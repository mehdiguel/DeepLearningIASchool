{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code du 12 ème du Classement leaderBord avec les commentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des libraires existants deja dans python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-pythonNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading opencv_python-4.4.0.46-cp38-cp38-win_amd64.whl (33.5 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\zenha\\anaconda3\\lib\\site-packages (from opencv-python) (1.18.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.4.0.46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import ImageFilter, Image \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from facenet_pytorch import MTCNN\n",
    "import torchvision \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Il a utilisé sys.path.append() & sys.path.insert() pour ajouter les chemins suivants dans la liste des repertoires dans  \n",
    "#### lesquels python cherche lorqu'un module est appelé \\ \n",
    "#### Remarque : dans chaque fichier des modules il ya un fichier '__init__.py' qui permet à Python de reconnaitre le dossier \n",
    "#### qui renferme ces fichiers (modules) comme un 'paquet' (librairie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "help(sys.path.append)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../input/efficientnet')\n",
    "sys.path.append('../input/imutils/imutils-0.5.3')\n",
    "sys.path.append('../input/dsfdinference/')\n",
    "sys.path.insert(0, \"/kaggle/input/blazeface-pytorch\")\n",
    "sys.path.insert(0, \"/kaggle/input/helpers\")\n",
    "sys.path.insert(0, \"/kaggle/input/timmmodels\")\n",
    "sys.path.insert(0,'/kaggle/working/reader/python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\zenha\\\\Desktop\\\\Alternance\\\\Cours\\\\Semestre_1\\\\Deep_learning\\\\DL_Projet',\n",
       " 'C:\\\\Users\\\\zenha\\\\anaconda3\\\\python38.zip',\n",
       " 'C:\\\\Users\\\\zenha\\\\anaconda3\\\\DLLs',\n",
       " 'C:\\\\Users\\\\zenha\\\\anaconda3\\\\lib',\n",
       " 'C:\\\\Users\\\\zenha\\\\anaconda3',\n",
       " '',\n",
       " 'C:\\\\Users\\\\zenha\\\\anaconda3\\\\lib\\\\site-packages',\n",
       " 'C:\\\\Users\\\\zenha\\\\anaconda3\\\\lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\Users\\\\zenha\\\\anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\Users\\\\zenha\\\\anaconda3\\\\lib\\\\site-packages\\\\Pythonwin',\n",
       " 'C:\\\\Users\\\\zenha\\\\anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions',\n",
       " 'C:\\\\Users\\\\zenha\\\\.ipython']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des modules externe à Python qui figurent dans les chemins ci-dessus qu'il a ajouté au path \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "#from decord import VideoReader as decord_VideoReader\n",
    "#from decord import cpu, gpu\n",
    "#from decord.bridge import set_bridge\n",
    "from imutils.video import FileVideoStream \n",
    "from efficientnet import EfficientNet\n",
    "#from dsfd.detect import DSFDDetector, get_face_detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0) # pour fixer les éléments choisi aléatoirement, dans le repertoir de base de python ( pour garder le meme échantion de test)\n",
    "np.random.seed(0) # pour fixer les éléments du random choisi dans Numpy\n",
    "torch.manual_seed(0) # pour fixer les éléments du random choisi dans Torch\n",
    "torch.cuda.manual_seed(0) # pour fixer les éléments du random choisi dans torch avec cuda\n",
    "torch.backends.cudnn.deterministic = True # s'assure que l'algorithme selectionné soit le meme a chaque fois que cuda est exécuté\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de la libraire de python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definir dans une variable test_dir le repertoire des videos test\n",
    "#  test_videos est une liste contenant les éléments du repertoir test_dir qui sont de format .mp4\n",
    "# afficher le nombre d'élements la liste test_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"/kaggle/input/deepfake-detection-challenge/test_videos/\"\n",
    "test_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])\n",
    "len(test_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afiicher la version de la library PyTorch\n",
    "# afficher les versions des modules cuda (permet de faire des calculs parallélisable plus rapidement uniquement des GPU)\n",
    "# afficher la version du backend de cudnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.4.0\n",
      "CUDA version: None\n",
      "cuDNN version: None\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il definit le type de device à 'cuda:0' si cuda est disponible (ou installé) et si non  type = 'cpu'\n",
    "# device est un périphérque\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#set_bridge('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import la class BlazeFace du  module BlazeFace \n",
    "## de la librairie pytorch.\n",
    "\n",
    "## BlazeFace explication:\n",
    "## Elle permet de détecter plusieurs visages sur une image de video\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blazeface import BlazeFace\n",
    "facedet = BlazeFace().to(device) # detecter les visages sur plusieurs images simultanement des batchs si gpu,  \n",
    "# batch par batch si cpu\n",
    "facedet.load_weights(\"/kaggle/input/blazeface-pytorch/blazeface.pth\") # charger des données du répertoir entre guillemet\n",
    "facedet.load_anchors(\"/kaggle/input/blazeface-pytorch/anchors.npy\") # charger des données du répertoir entre guillemet\n",
    "_ = facedet.train(False) # ne pas faire de detection sur les données d'entrainements stocké dans la variable underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la taille en entré des images des videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import du Module Normalize qui permet de normaliser les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Normalize\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "normalize_transform = Normalize(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonctions:\n",
    "# - disable_grad : qui prend en entrée le model, et ne calcule pas les gradients des parametres ensuite il renvoie le modele\n",
    "\n",
    "# - normalize : prend en entrée une image, il renvoie une image redimensionnée\n",
    "\n",
    "# - weight_preds : prends en entrée les predictions et les poids, et renvoie un tensor.\n",
    "\n",
    "# - predict_face : prends en entrée le modele, les données, le poids et n(nb de variables) , et renvoie la prediction d'un visage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_grad(model):\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "def normalize(img):\n",
    "    y, x, _ = img.shape\n",
    "    \n",
    "    if y > x and x < 256:\n",
    "        ratio_x = x / y\n",
    "        ratio_y = y / x\n",
    "\n",
    "        return cv2.resize(img, (256, int(ratio_y * 256)))\n",
    "    elif y < x and y < 256:\n",
    "        ratio_x = x / y\n",
    "        ratio_y = y / x\n",
    "\n",
    "        return cv2.resize(img, (int(ratio_x * 256), 256))\n",
    "    else:\n",
    "        return cv2.resize(img, (256, 256))\n",
    "        \n",
    "\n",
    "def weight_preds(preds, weights):\n",
    "    final_preds = []\n",
    "    for i in range(len(preds)):\n",
    "        for j in range(len(preds[i])):\n",
    "            if len(final_preds) != len(preds[i]):\n",
    "                final_preds.append(preds[i][j] * weights[i])\n",
    "            else:\n",
    "                final_preds[j] += preds[i][j] * weights[i]\n",
    "                \n",
    "    return torch.FloatTensor(final_preds)\n",
    "\n",
    "\n",
    "def predict_faces(models, x, weigths, n):\n",
    "    x = torch.tensor(x, device=device).float()\n",
    "\n",
    "    # Preprocess the images.\n",
    "    x = x.permute((0, 3, 1, 2))\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        x[i] = normalize_transform(x[i] / 255.)\n",
    "\n",
    "    # Make a prediction, then take the average.\n",
    "    with torch.no_grad():\n",
    "        y_pred = 0\n",
    "        preds = []\n",
    "        for i in range(len(models)):\n",
    "            preds.append(models[i](x).squeeze()[:n])\n",
    "        \n",
    "        del x\n",
    "        \n",
    "        y_pred = torch.sigmoid(weight_preds(preds, weigths)).mean().item()\n",
    "\n",
    "        return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import les modules qu'il a cré lui meme pour lire les videos et extraire les visage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.read_video_1 import VideoReader\n",
    "from helpers.face_extract_1 import FaceExtractor\n",
    "\n",
    "frames_per_video = 32\n",
    "\n",
    "video_reader = VideoReader()\n",
    "video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video) #get_frames(x, batch_size=frames_per_video)\n",
    "face_extractor = FaceExtractor(video_read_fn, facedet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import tensorflow as tf\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.io.gfile.GFile('../input/mobilenet-face/frozen_inference_graph_face.pb', 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "        config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess=tf.compat.v1.Session(graph=detection_graph, config=config)\n",
    "    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "    boxes_tensor = detection_graph.get_tensor_by_name('detection_boxes:0')    \n",
    "    scores_tensor = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "    num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "    \n",
    "    \n",
    "def get_mobilenet_face(image):\n",
    "    global boxes,scores,num_detections\n",
    "    (im_height,im_width)=image.shape[:-1]\n",
    "    imgs=np.array([image])\n",
    "    (boxes, scores) = sess.run(\n",
    "        [boxes_tensor, scores_tensor],\n",
    "        feed_dict={image_tensor: imgs})\n",
    "    max_=np.where(scores==scores.max())[0][0]\n",
    "    box=boxes[0][max_]\n",
    "    ymin, xmin, ymax, xmax = box\n",
    "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                ymin * im_height, ymax * im_height)\n",
    "    left, right, top, bottom = int(left), int(right), int(top), int(bottom)\n",
    "    return (left, right, top, bottom)\n",
    "\n",
    "def crop_image(frame,bbox):\n",
    "    left, right, top, bottom=bbox\n",
    "    return frame[top:bottom,left:right]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class qu'il appel à la sortie d'un modele sur la derniere couche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaModel(nn.Module):\n",
    "    def __init__(self, models=None, device='cuda:0', extended=False):\n",
    "        super(MetaModel, self).__init__()\n",
    "        \n",
    "        self.extended = extended\n",
    "        self.device = device\n",
    "        self.models = models\n",
    "        self.len = len(models)\n",
    "        \n",
    "        if self.extended:\n",
    "            self.bn = nn.BatchNorm1d(self.len)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc = nn.Linear(self.len, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.cat(tuple(x), dim=1)\n",
    "        \n",
    "        if self.extended:\n",
    "            x = self.bn(x)\n",
    "            x = self.relu(x)\n",
    "            #x = self.dropout(x)\n",
    "            \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupération des modeles deja entrainé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = \"/kaggle/input/deepfake-detection-model-20k/\" # affecte à la variable le chemin des modèle\n",
    "WEIGTHS_EXT = '.pth' # l'extension des poids\n",
    "\n",
    "models = []\n",
    "weigths = []\n",
    "    \n",
    "raw_data_stack = \\\n",
    "[\n",
    "    ['0.8548137313946486 0.3376769562025044', 'efficientnet-b2'],\n",
    "    ['EfficientNetb3 0.8573518024606384 0.34558522378585194', 'efficientnet-b3'],\n",
    "    ['EfficientNetb4 0.8579110384582294 0.3383911053075265', 'efficientnet-b4'],\n",
    "    ['EfficientNet6 0.8602770369095758 0.33193617861157143', 'efficientnet-b6'],\n",
    "    ['EfficientNetb0 t2 0.8616966359803837 0.3698434531609828', 'efficientnet-b0'],\n",
    "    ['EfficientNetb1 t2 0.8410909403768391 0.36058002083572327', 'efficientnet-b1'],\n",
    "    ['EfficientNetb2 t2 0.8659554331928073 0.35598630783834084', 'efficientnet-b2'],\n",
    "    ['EfficientNetb3 t2 0.8486191172674868 0.3611779548592305', 'efficientnet-b3'],\n",
    "    ['EfficientNetb3 0.8635894347414609 0.328333642473084', 'efficientnet-b3'],\n",
    "    ['EfficientNetb6 0.8593736556826981 0.32286693639934694', 'efficientnet-b6'],\n",
    "    ['tf_efficientnet_b1_ns 0.8571367116923342 0.3341234226295108', 'tf_efficientnet_b1_ns'],\n",
    "    ['tf_efficientnet_b3_ns 0.8712466660930913 0.3277394129117183', 'tf_efficientnet_b3_ns'],\n",
    "    ['tf_efficientnet_b4_ns 0.8708595027101437 0.3152573955405342', 'tf_efficientnet_b4_ns'],\n",
    "    ['tf_efficientnet_b6_ns 0.8733115374688118 0.3156576980666498', 'tf_efficientnet_b6_ns'],\n",
    "]\n",
    "\n",
    "stack_models = []\n",
    "for raw_model in raw_data_stack:\n",
    "    checkpoint = torch.load( MODELS_PATH + raw_model[0] + WEIGTHS_EXT, map_location=device)\n",
    "    \n",
    "    if '-' in raw_model[1]:\n",
    "        model = EfficientNet.from_name(raw_model[1])\n",
    "        model._fc = nn.Linear(model._fc.in_features, 1)\n",
    "    else:\n",
    "        model = timm.create_model(raw_model[1], pretrained=False)\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, 1)\n",
    "    \n",
    "    model.load_state_dict(checkpoint)\n",
    "    _ = model.eval()\n",
    "    _ = disable_grad(model)\n",
    "    model = model.to(device)\n",
    "    stack_models.append(model)\n",
    "\n",
    "    del checkpoint, model\n",
    "    \n",
    "\n",
    "meta_models = \\\n",
    "[\n",
    "    ['MetaModel 0.30638167556896007', slice(4, 8), False, 0.37780],\n",
    "    ['MetaModel 0.2919331893755284', slice(0, 4), False, 0.33357],\n",
    "    ['MetaModel 0.30281482560578044', slice(0, 8, None), True, 0.34077],\n",
    "    ['MetaModel 0.26302117601197256', slice(0, 10, None), False, 0.35134],\n",
    "    ['MetaModel 0.256337642808031', slice(10, 14, None), False, 0.32698],\n",
    "    ['MetaModel 0.264787397152165', slice(0, 14, None), False, 0.34974]\n",
    "]\n",
    "\n",
    "for meta_raw in meta_models:\n",
    "\n",
    "    checkpoint = torch.load(MODELS_PATH + meta_raw[0] + WEIGTHS_EXT, map_location=device)\n",
    "    \n",
    "    model = MetaModel(models=raw_data_stack[meta_raw[1]], extended=meta_raw[2]).to(device)\n",
    "    \n",
    "    #model = MetaModel(models=stack_models[meta_raw[1]], extended=meta_raw[2]).to(device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint)\n",
    "    _ = model.eval()\n",
    "    _ = disable_grad(model)\n",
    "    model.to(device)\n",
    "    models.append(model)\n",
    "    weigths.append(meta_raw[3])\n",
    "\n",
    "    del model, checkpoint\n",
    "    \n",
    "total = sum([1-score for score in weigths])\n",
    "weigths = [(1-score) / total for score in weigths]\n",
    "\n",
    "'''checkpoint = torch.load(MODELS_PATH + 'MetaModel 0.256337642808031.pth', map_location=device)\n",
    "meta = MetaModel(stack_models).to(device)\n",
    "meta.load_state_dict(checkpoint)\n",
    "_ = meta.eval()\n",
    "_ = disable_grad(meta)\n",
    "\n",
    "del checkpoint'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédire sur une video si elle est fausse ou vrai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "def predict_on_video(video_path, batch_size):\n",
    "    try:\n",
    "        # Find the faces for N frames in the video.\n",
    "        faces = face_extractor.process_video(video_path)\n",
    "\n",
    "        # Only look at one face per frame.\n",
    "        face_extractor.keep_only_best_face(faces)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            # NOTE: When running on the CPU, the batch size must be fixed\n",
    "            # or else memory usage will blow up. (Bug in PyTorch?)\n",
    "            x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8)\n",
    "\n",
    "            # If we found any faces, prepare them for the model.\n",
    "            n = 0\n",
    "            for frame_data in faces:\n",
    "                for face in frame_data[\"faces\"]:\n",
    "                    # Resize to the model's required input size.\n",
    "                    # We keep the aspect ratio intact and add zero\n",
    "                    # padding if necessary.\n",
    "\n",
    "                    #resized_face = isotropically_resize_image(face, input_size)\n",
    "                    #resized_face = make_square_image(resized_face)\n",
    "                    \n",
    "                    resized_face = normalize(face)\n",
    "                    resized_face = torchvision.transforms.CenterCrop((input_size, input_size))(Image.fromarray(resized_face))\n",
    "                    #resized_face = cv2.resize(face, (input_size, input_size))\n",
    "                    \n",
    "                    if n < batch_size:\n",
    "                        x[n] = resized_face\n",
    "                        n += 1\n",
    "                    else:\n",
    "                        print(\"WARNING: have %d faces but batch size is %d\" % (n, batch_size))\n",
    "\n",
    "                    # Test time augmentation: horizontal flips.\n",
    "                    # TODO: not sure yet if this helps or not\n",
    "                    #x[n] = cv2.flip(resized_face, 1)\n",
    "                    #n += 1\n",
    "\n",
    "\n",
    "            del faces\n",
    "\n",
    "            if n > 0:\n",
    "                x = torch.tensor(x, device=device).float()\n",
    "\n",
    "                # Preprocess the images.\n",
    "                x = x.permute((0, 3, 1, 2))\n",
    "\n",
    "                for i in range(len(x)):\n",
    "                    x[i] = normalize_transform(x[i] / 255.)\n",
    "\n",
    "                # Make a prediction, then take the average.\n",
    "                with torch.no_grad():\n",
    "                    y_pred = 0\n",
    "                    stacked_preds = []\n",
    "                    preds = []\n",
    "                    \n",
    "                    for i in range(len(stack_models)):\n",
    "                        stacked_preds.append(stack_models[i](x).squeeze()[:n].unsqueeze(dim=1))\n",
    "\n",
    "                    \n",
    "                    for i in range(len(models)):\n",
    "                        preds.append(models[i](stacked_preds[meta_models[i][1]]))\n",
    "                \n",
    "                    del x, stacked_preds\n",
    "                    \n",
    "                    y_pred = torch.sigmoid(weight_preds(preds, weigths)).mean().item() #torch.sigmoid(metav4(preds)).mean().item()\n",
    "                    \n",
    "                    del preds\n",
    "                    \n",
    "                    return y_pred\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n",
    "    \n",
    "    \n",
    "    return 0.5#predict_mobilenet(video_path, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### faire la prédiction sur une video seule si elle est fausse ou vrai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_video_single(video_path, batch_size):\n",
    "    try:\n",
    "        # Find the faces for N frames in the video.\n",
    "        faces = face_extractor.process_video(video_path)\n",
    "\n",
    "        # Only look at one face per frame.\n",
    "        face_extractor.keep_only_best_face(faces)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            # NOTE: When running on the CPU, the batch size must be fixed\n",
    "            # or else memory usage will blow up. (Bug in PyTorch?)\n",
    "            x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8)\n",
    "\n",
    "            # If we found any faces, prepare them for the model.\n",
    "            n = 0\n",
    "            for frame_data in faces:\n",
    "                for face in frame_data[\"faces\"]:\n",
    "                    # Resize to the model's required input size.\n",
    "                    # We keep the aspect ratio intact and add zero\n",
    "                    # padding if necessary.\n",
    "\n",
    "                    #resized_face = isotropically_resize_image(face, input_size)\n",
    "                    #resized_face = make_square_image(resized_face)\n",
    "                    \n",
    "                     #resized_face = torchvision.transforms.Resize((input_size, input_size))(Image.fromarray(face))\n",
    "                    resized_face = cv2.resize(face, (input_size, input_size))\n",
    "                    if n < batch_size:\n",
    "                        x[n] = resized_face\n",
    "                        n += 1\n",
    "                    else:\n",
    "                        print(\"WARNING: have %d faces but batch size is %d\" % (n, batch_size))\n",
    "\n",
    "                    # Test time augmentation: horizontal flips.\n",
    "                    # TODO: not sure yet if this helps or not\n",
    "                    #x[n] = cv2.flip(resized_face, 1)\n",
    "                    #n += 1\n",
    "\n",
    "            del faces\n",
    "\n",
    "            if n > 0:\n",
    "                x = torch.tensor(x, device=device).float()\n",
    "\n",
    "                # Preprocess the images.\n",
    "                x = x.permute((0, 3, 1, 2))\n",
    "                \n",
    "                for i in range(len(x)):\n",
    "                    x[i] = normalize_transform(x[i] / 255.)\n",
    "\n",
    "                # Make a prediction, then take the average.\n",
    "                with torch.no_grad():\n",
    "                    stacked_preds = []\n",
    "                    preds = []\n",
    "                    \n",
    "                    for i in range(len(stack_models)):\n",
    "                        stacked_preds.append(stack_models[i](x).squeeze()[:n].unsqueeze(dim=1))\n",
    "                    \n",
    "                    del x\n",
    "                    \n",
    "                    y_pred = torch.sigmoid(models[-1](stacked_preds)).mean().item()\n",
    "\n",
    "                    return y_pred\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n",
    "    \n",
    "    return 0.5#predict_mobilenet(video_path, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prédire sur plusieurs videos si elles sont fausses ou vraies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import gc\n",
    "\n",
    "def predict_on_video_set(videos, num_workers):\n",
    "    def process_file(i):\n",
    "        filename = videos[i]\n",
    "        y_pred = predict_on_video(os.path.join(test_dir, filename), batch_size=frames_per_video)\n",
    "        \n",
    "        '''if y_pred > 0.95:\n",
    "            y_pred = 0.95\n",
    "        elif y_pred < 0.05:\n",
    "            y_pred = 0.05'''\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n",
    "        predictions = ex.map(process_file, range(len(videos)))\n",
    "        \n",
    "    return list(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_test = False  # you have to enable this manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elapsed 6.873434 min. Average per video: 8.248120 sec.\n",
    "if speed_test:\n",
    "    start_time = time.time()\n",
    "    speedtest_videos = test_videos[:5]\n",
    "    predictions = predict_on_video_set(speedtest_videos, num_workers=4)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Elapsed %f min. Average per video: %f sec.\" % (elapsed / 60, elapsed / len(speedtest_videos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_on_video_set(test_videos, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du fichier données qu'il va soumettre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\"filename\": test_videos, \"label\": predictions})\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r reader && rm install.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
